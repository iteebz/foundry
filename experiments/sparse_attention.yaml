name: "sparse_attention"

training:
  max_iters: 5000
  learning_rate: 6e-4
  eval_interval: 500
  compile: false

data:
  dataset: "shakespeare_char"
  batch_size: 64
  block_size: 256

model_args:
  n_layer: 6
  n_head: 6
  n_kv_head: 2
  n_embd: 384
  dropout: 0.0
  bias: false
  norm_type: "rmsnorm"
  activation: "swiglu"
  position_encoding: "rope"
  attention_type: "gqa"
  sparse_block_size: 64
  sparse_stride: 64
